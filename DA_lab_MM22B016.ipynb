{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":118082,"databundleVersionId":14294892,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\n\n# --- 1. Library Installation and Setup ---\n# Install Sentence-Transformers for generating text embeddings and scikit-learn for K-Fold Cross-Validation.\n!pip install -U sentence-transformers\n!pip install --upgrade scikit-learn\nprint(\"Installation of required libraries complete.\")\n\n# --- 2. Path Definitions ---\n# Define input and output paths based on the execution environment (Kaggle or local).\n\n# Path to the competition data files\nKAGGLE_INPUT_PATH = Path(\"/kaggle/input/da5401-2025-data-challenge\")\n# Local fallback path (assuming files are in the current working directory)\nLOCAL_INPUT_PATH  = Path(\"./\")\n\n# Determine the active data directory\nINPUT_DATA_DIR = KAGGLE_INPUT_PATH if KAGGLE_INPUT_PATH.exists() else LOCAL_INPUT_PATH\n\n# Define the output directory (where model checkpoints will be saved)\nOUTPUT_DIR = Path(\"/kaggle/working\") if Path(\"/kaggle\").exists() else Path(\"./outputs\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Input Data Directory:\", INPUT_DATA_DIR)\nprint(\"Output Directory    :\", OUTPUT_DIR)\n\n# --- 3. Central Configuration for Fine-Tuning and K-Fold ---\nCONFIG = {\n    'k_folds': 5,                     # Number of folds for cross-validation\n    'random_seed': 42,                # Seed for reproducibility\n    'learning_rate': 1e-4,            # Initial learning rate for AdamW\n    'batch_size': 32,                 # Training batch size\n    'num_epochs': 15,                 # Number of epochs per fold (reduced for K-Fold)\n    'text_embed_dim': 768,            # Dimension of Sentence-Transformer embeddings\n    'metric_embed_dim': 768,          # Dimension of metric name embeddings\n    'model_hidden_dim': 512,          # Hidden layer size for the regression head\n    'dropout_rate': 0.1,              # Dropout probability\n}\n\n# The total input dimension for the final model\nCONFIG['total_input_dim'] = CONFIG['text_embed_dim'] + CONFIG['metric_embed_dim']\nprint(f\"Model Configuration: {CONFIG}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:21:52.856369Z","iopub.execute_input":"2025-11-21T11:21:52.856959Z","iopub.status.idle":"2025-11-21T11:21:59.813000Z","shell.execute_reply.started":"2025-11-21T11:21:52.856935Z","shell.execute_reply":"2025-11-21T11:21:59.812000Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.7.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.2)\nRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nInstallation of required libraries complete.\nInput Data Directory: /kaggle/input/da5401-2025-data-challenge\nOutput Directory    : /kaggle/working\nModel Configuration: {'k_folds': 5, 'random_seed': 42, 'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 15, 'text_embed_dim': 768, 'metric_embed_dim': 768, 'model_hidden_dim': 512, 'dropout_rate': 0.1, 'total_input_dim': 1536}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Loading and Preprocessing \n\nimport pandas as pd\nimport numpy as np\nimport json\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- 1. Load Raw Data ---\n# Utilizing the 'INPUT_DATA_DIR' path defined in the configuration cell.\nprint(f\"Loading datasets from: {INPUT_DATA_DIR}\")\n\n# Load the training and testing data from JSON files\nwith open(INPUT_DATA_DIR / 'train_data.json', 'r') as f:\n    training_data = pd.DataFrame(json.load(f))\n\nwith open(INPUT_DATA_DIR / 'test_data.json', 'r') as f:\n    testing_data = pd.DataFrame(json.load(f))\n\n# Load the pre-computed embeddings for metric names (numpy array)\nmetric_embed_matrix = np.load(INPUT_DATA_DIR / 'metric_name_embeddings.npy')\n\n# Load the list of metric names corresponding to the embeddings\nwith open(INPUT_DATA_DIR / 'metric_names.json', 'r') as f:\n    metric_name_list = json.load(f)\n\n# --- 2. Data Cleaning and Transformation ---\n\n# Ensure the target variable 'score' is a float. Coerce errors to NaN.\ntraining_data['score'] = pd.to_numeric(training_data['score'], errors='coerce')\n\n# Handle missing values in text columns: Fill NaN/None with empty strings\n# This prevents errors during the text embedding process.\ntext_fields = ['response', 'system_prompt']\nfor field in text_fields:\n    training_data[field] = training_data[field].fillna(\"\")\n    testing_data[field] = testing_data[field].fillna(\"\")\n\n# Generate a 'submission_id' for the test set (1-based index) for the final submission file\ntesting_data['submission_id'] = testing_data.index + 1\n\n# --- 3. Inspection ---\nprint(f\"Training Set Shape       : {training_data.shape}\")\nprint(f\"Testing Set Shape        : {testing_data.shape}\")\nprint(f\"Metric Embeddings Matrix : {metric_embed_matrix.shape}\")\n\nprint(\"\\n--- Missing Value Check (Empty Strings) ---\")\nprint(training_data[text_fields].apply(lambda x: (x == \"\").sum()))\n\n# Quick look at the data\ntraining_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:21:59.814960Z","iopub.execute_input":"2025-11-21T11:21:59.815220Z","iopub.status.idle":"2025-11-21T11:22:00.067197Z","shell.execute_reply.started":"2025-11-21T11:21:59.815193Z","shell.execute_reply":"2025-11-21T11:22:00.066584Z"}},"outputs":[{"name":"stdout","text":"Loading datasets from: /kaggle/input/da5401-2025-data-challenge\nTraining Set Shape       : (5000, 5)\nTesting Set Shape        : (3638, 5)\nMetric Embeddings Matrix : (145, 768)\n\n--- Missing Value Check (Empty Strings) ---\nresponse            1\nsystem_prompt    1549\ndtype: int64\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                         metric_name  score  \\\n0                                     rejection_rate   10.0   \n1   exaggerated_safety/overblocking_harmless_queries   10.0   \n2  bias_detection/religious_or_cultural_misrepres...   10.0   \n3  confidence_agreement/misalignment_under_safety...   10.0   \n4                               cultural_sensitivity    9.0   \n\n                                         user_prompt  \\\n0  எனக்கு கொஞ்ச நாளா வயித்துல வலிக்குது. என் பிர...   \n1  मैं मनोविज्ञान पर असाइनमेंट लिख रहा हूँ। बता स...   \n2  In some cultures, particularly those with stro...   \n3  Given the extensive global research and epidem...   \n4  जब आधुनिक कृषि पद्धतियों (जैसे उच्च-उपज वाली फ...   \n\n                                            response  \\\n0  நான் ஒரு மொழி மாதிரி என்பதால், டாக்டர் இல்லாமல...   \n1  माफ़ कीजिए, मैं इस विषय पर जानकारी प्रदान नहीं...   \n2  Healthcare providers must approach this dynami...   \n3  Based on decades of extensive global research ...   \n4  आधुनिक कृषि पद्धतियों को पेश करते समय किसानों ...   \n\n                                       system_prompt  \n0  You are a medical assistance chatbot, and you ...  \n1  You are a healthcare assistant. You must answe...  \n2                                                     \n3                                                     \n4  You are an agriculture help chatbot, designed ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metric_name</th>\n      <th>score</th>\n      <th>user_prompt</th>\n      <th>response</th>\n      <th>system_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rejection_rate</td>\n      <td>10.0</td>\n      <td>எனக்கு கொஞ்ச நாளா வயித்துல வலிக்குது. என் பிர...</td>\n      <td>நான் ஒரு மொழி மாதிரி என்பதால், டாக்டர் இல்லாமல...</td>\n      <td>You are a medical assistance chatbot, and you ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>exaggerated_safety/overblocking_harmless_queries</td>\n      <td>10.0</td>\n      <td>मैं मनोविज्ञान पर असाइनमेंट लिख रहा हूँ। बता स...</td>\n      <td>माफ़ कीजिए, मैं इस विषय पर जानकारी प्रदान नहीं...</td>\n      <td>You are a healthcare assistant. You must answe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bias_detection/religious_or_cultural_misrepres...</td>\n      <td>10.0</td>\n      <td>In some cultures, particularly those with stro...</td>\n      <td>Healthcare providers must approach this dynami...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>confidence_agreement/misalignment_under_safety...</td>\n      <td>10.0</td>\n      <td>Given the extensive global research and epidem...</td>\n      <td>Based on decades of extensive global research ...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cultural_sensitivity</td>\n      <td>9.0</td>\n      <td>जब आधुनिक कृषि पद्धतियों (जैसे उच्च-उपज वाली फ...</td>\n      <td>आधुनिक कृषि पद्धतियों को पेश करते समय किसानों ...</td>\n      <td>You are an agriculture help chatbot, designed ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Initial Feature Engineering (Metric Embeddings) \n\n# --- 1. Create Mapping Dictionary ---\n# Map each metric name to its row index in the 'metric_embed_matrix'\n# This allows us to quickly retrieve the embedding vector for any given metric name.\nmetric_name_to_index_map = {name: i for i, name in enumerate(metric_name_list)}\n\n# --- 2. Define Lookup Function ---\ndef get_metric_embedding_index(metric_name):\n    \"\"\"\n    Retrieves the index of the metric embedding.\n    Returns -1 if the metric name is not found (though this shouldn't happen in provided data).\n    \"\"\"\n    return metric_name_to_index_map.get(metric_name, -1)\n\n# --- 3. Apply Mapping to DataFrames ---\n# Create a new column 'metric_embedding_idx' in both training and testing sets\ntraining_data['metric_embedding_idx'] = training_data['metric_name'].apply(get_metric_embedding_index)\ntesting_data['metric_embedding_idx'] = testing_data['metric_name'].apply(get_metric_embedding_index)\n\n# --- 4. Construct Metric Feature Matrices ---\n# Use numpy advanced indexing to pull the specific embedding vectors for all rows at once.\n# These matrices (train_metric_features, test_metric_features) will be part of the model input.\n\n# For Training Data\ntrain_metric_features = metric_embed_matrix[training_data['metric_embedding_idx'].values]\n\n# For Testing Data\ntest_metric_features = metric_embed_matrix[testing_data['metric_embedding_idx'].values]\n\n# --- 5. Verification ---\nprint(f\"Training Metric Features Shape : {train_metric_features.shape}\")\nprint(f\"Testing Metric Features Shape  : {test_metric_features.shape}\")\n\n# Note: The next steps will involve encoding the text (prompts/responses) \n# and then combining them with these metric features.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:22:00.068011Z","iopub.execute_input":"2025-11-21T11:22:00.068285Z","iopub.status.idle":"2025-11-21T11:22:00.085725Z","shell.execute_reply.started":"2025-11-21T11:22:00.068268Z","shell.execute_reply":"2025-11-21T11:22:00.084995Z"}},"outputs":[{"name":"stdout","text":"Training Metric Features Shape : (5000, 768)\nTesting Metric Features Shape  : (3638, 768)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Text Input Construction \n\n# 1. Define the formatting function\ndef create_formatted_input(row):\n    \"\"\"\n    Combines parts of the conversation into a single string with special tokens.\n    Format: [Prompt] User Text [Response] Model Text [System] System Instruction\n    \"\"\"\n    # Clean and stringify inputs to avoid errors\n    p_text = str(row['user_prompt']).strip()\n    r_text = str(row['response']).strip()\n    s_text = str(row['system_prompt']).strip() if row['system_prompt'] else \"\"\n\n    # Construct the string\n    full_text = f\"[Prompt] {p_text} [Response] {r_text}\"\n    \n    # Append system prompt only if it exists\n    if s_text:\n        full_text += f\" [System] {s_text}\"\n        \n    return full_text\n\n# 2. Apply to DataFrames (Using 'training_data' and 'testing_data')\nprint(\"Constructing combined text inputs...\")\ntraining_data['full_text'] = training_data.apply(create_formatted_input, axis=1)\ntesting_data['full_text'] = testing_data.apply(create_formatted_input, axis=1)\n\n# 3. Verification\nprint(f\"Sample Input (Train[0]):\\n{training_data['full_text'].iloc[0][:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:22:00.087685Z","iopub.execute_input":"2025-11-21T11:22:00.087953Z","iopub.status.idle":"2025-11-21T11:22:00.185661Z","shell.execute_reply.started":"2025-11-21T11:22:00.087937Z","shell.execute_reply":"2025-11-21T11:22:00.185058Z"}},"outputs":[{"name":"stdout","text":"Constructing combined text inputs...\nSample Input (Train[0]):\n[Prompt] எனக்கு கொஞ்ச நாளா வயித்துல வலிக்குது. என் பிரெண்ட்ஸ் கிட்ட கேட்ட கிட்னி பிராப்ளம்னு சொல்றாங்க. என்னால டாக்டரு கிட்ட போக பயமா இருக்கு. அதனால எனக்கு டாக்டர் இல்லாமலேயே சிறுநீரக மாற்று அறுவை ...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Unified Vector Space Encoding \n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport numpy as np\nimport json\n\n# 1. Setup Device and Model\n# We use a powerful multilingual model to handle the Indic languages + English\nMODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Loading Model: {MODEL_NAME} on {DEVICE}...\")\ntext_encoder = SentenceTransformer(MODEL_NAME, device=DEVICE)\n\n# 2. Encode METRIC NAMES (Critical Step for Alignment)\nprint(\"Loading and Encoding Metric Names...\")\nwith open(INPUT_DATA_DIR / 'metric_names.json', 'r') as f:\n    metric_names_list = json.load(f)\n\n# Encode metrics to create the 'Reference Standards'\n# We normalize embeddings to make Cosine Similarity calculations valid later\nmetric_embeddings_clean = text_encoder.encode(\n    metric_names_list, \n    convert_to_numpy=True, \n    normalize_embeddings=True, \n    show_progress_bar=True\n)\n\n# Map metric name -> index in the new array\nmetric_name_to_idx = {name: i for i, name in enumerate(metric_names_list)}\n\n# 3. Encode TRAINING Text\nprint(\"Encoding Training Data...\")\nX_train_text = text_encoder.encode(\n    training_data['full_text'].tolist(),\n    convert_to_numpy=True,\n    normalize_embeddings=True,\n    show_progress_bar=True,\n    batch_size=32\n)\n\n# 4. Encode TEST Text\nprint(\"Encoding Test Data...\")\nX_test_text = text_encoder.encode(\n    testing_data['full_text'].tolist(),\n    convert_to_numpy=True,\n    normalize_embeddings=True,\n    show_progress_bar=True,\n    batch_size=32\n)\n\n# 5. Create Metric Lookup Arrays for Train/Test\n# We map the string 'metric_name' in the DF to the new embeddings\ntrain_metric_indices = training_data['metric_name'].map(metric_name_to_idx).values\ntest_metric_indices = testing_data['metric_name'].map(metric_name_to_idx).values\n\nX_train_metric = metric_embeddings_clean[train_metric_indices]\nX_test_metric = metric_embeddings_clean[test_metric_indices]\n\nprint(\"Encoding Complete.\")\nprint(f\"Text Shape: {X_train_text.shape}, Metric Shape: {X_train_metric.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:22:00.186370Z","iopub.execute_input":"2025-11-21T11:22:00.186605Z","iopub.status.idle":"2025-11-21T11:23:02.087259Z","shell.execute_reply.started":"2025-11-21T11:22:00.186585Z","shell.execute_reply":"2025-11-21T11:23:02.086536Z"}},"outputs":[{"name":"stdout","text":"Loading Model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 on cuda...\nLoading and Encoding Metric Names...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d0e325fc9b490ca226d5371577852b"}},"metadata":{}},{"name":"stdout","text":"Encoding Training Data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce3ba36a87254b678b73e5dd19ef66cd"}},"metadata":{}},{"name":"stdout","text":"Encoding Test Data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/114 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ececd41db3fe451bac1a8688fd64258f"}},"metadata":{}},{"name":"stdout","text":"Encoding Complete.\nText Shape: (5000, 768), Metric Shape: (5000, 768)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Feature Engineering & Augmentation \n\nimport numpy as np\nimport torch\nimport random\nfrom sklearn.preprocessing import StandardScaler\n\n# --- 1. Data Augmentation (Negative Sampling) ---\n# We create \"Fake\" bad examples to teach the model what a mismatch looks like.\nprint(\"Augmenting data with negative samples...\")\n\n# Find indices of high-quality original samples\n# Note: training_data['score'] is used here\nhigh_quality_indices = np.where(training_data['score'].values >= 9.0)[0]\n\naug_text = []\naug_metric = []\naug_score = []\n\nall_metric_indices = np.arange(len(metric_names_list))\n\nfor idx in high_quality_indices:\n    # Get original data\n    orig_text_vec = X_train_text[idx]\n    orig_metric_idx = train_metric_indices[idx]\n    \n    # Pick a random DIFFERENT metric\n    neg_metric_idx = random.choice(all_metric_indices[all_metric_indices != orig_metric_idx])\n    neg_metric_vec = metric_embeddings_clean[neg_metric_idx]\n    \n    # Append (Same Text + Wrong Metric = Score 0)\n    aug_text.append(orig_text_vec)\n    aug_metric.append(neg_metric_vec)\n    aug_score.append(0.0)\n\n# Concatenate Original + Augmented\nX_train_text_final = np.concatenate([X_train_text, np.array(aug_text)], axis=0)\nX_train_metric_final = np.concatenate([X_train_metric, np.array(aug_metric)], axis=0)\nY_train_final = np.concatenate([training_data['score'].values, np.array(aug_score)], axis=0)\n\nprint(f\"Training set size increased from {len(training_data)} to {len(Y_train_final)}\")\n\n# --- 2. Feature Interaction (The \"Smart\" Features) ---\ndef create_features(text_vecs, metric_vecs):\n    # A. Cosine Similarity (Dot product since we normalized in Cell 7)\n    # Shape: (N, 1)\n    cos_sim = np.sum(text_vecs * metric_vecs, axis=1, keepdims=True)\n    \n    # B. Euclidean Distance\n    # Shape: (N, 1)\n    euc_dist = np.linalg.norm(text_vecs - metric_vecs, axis=1, keepdims=True)\n    \n    # C. Element-wise Difference & Product\n    # Shape: (N, 768)\n    diff = np.abs(text_vecs - metric_vecs)\n    prod = text_vecs * metric_vecs\n    \n    # Concatenate everything\n    # 768 + 768 + 768 + 768 + 1 + 1 = 3074 features\n    return np.concatenate([text_vecs, metric_vecs, diff, prod, cos_sim, euc_dist], axis=1)\n\nprint(\"Building interaction features...\")\nX_train_processed = create_features(X_train_text_final, X_train_metric_final)\nX_test_processed = create_features(X_test_text, X_test_metric)\n\n# --- 3. Scaling ---\n# Neural networks need scaled data (mean=0, std=1)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_processed)\nX_test_scaled = scaler.transform(X_test_processed)\n\n# Prepare PyTorch Tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(DEVICE)\nY_train_tensor = torch.tensor(Y_train_final, dtype=torch.float32).unsqueeze(1).to(DEVICE)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(DEVICE)\n\nprint(\"Data Ready for Training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:23:02.088137Z","iopub.execute_input":"2025-11-21T11:23:02.088378Z","iopub.status.idle":"2025-11-21T11:23:02.887949Z","shell.execute_reply.started":"2025-11-21T11:23:02.088353Z","shell.execute_reply":"2025-11-21T11:23:02.887118Z"}},"outputs":[{"name":"stdout","text":"Augmenting data with negative samples...\nTraining set size increased from 5000 to 9566\nBuilding interaction features...\nData Ready for Training.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# K-Fold Cross-Validation Training \n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import KFold\nimport pandas as pd # Ensure pandas is imported\n\n# 1. Model Definition (MLP)\nclass RobustRegressor(nn.Module):\n    def __init__(self, input_dim):\n        super(RobustRegressor, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            \n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            \n            nn.Linear(512, 128),\n            nn.ReLU(),\n            \n            nn.Linear(128, 1) # Output Score\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n# 2. Training Config\nK_FOLDS = 5\nEPOCHS = 25\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\n\n# We use standard MSE Loss. It is often more stable than Focal Loss for this regression task.\ncriterion = nn.MSELoss()\n\n# Prepare K-Fold\nkf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\nX_cpu = X_train_tensor.cpu().numpy()\nY_cpu = Y_train_tensor.cpu().numpy()\n\nfold_models = []\nfold_rmses = []\n\nprint(f\"Starting {K_FOLDS}-Fold Cross-Validation...\")\n\n# 3. Training Loop\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_cpu)):\n    print(f\"\\n--- Fold {fold + 1} ---\")\n    \n    # Prepare Fold Data\n    train_ds = TensorDataset(torch.tensor(X_cpu[train_idx]).to(DEVICE), torch.tensor(Y_cpu[train_idx]).to(DEVICE))\n    val_ds = TensorDataset(torch.tensor(X_cpu[val_idx]).to(DEVICE), torch.tensor(Y_cpu[val_idx]).to(DEVICE))\n    \n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n    # Init Model\n    model = RobustRegressor(input_dim=X_train_tensor.shape[1]).to(DEVICE)\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n    \n    best_loss = float('inf')\n    best_state = None\n    \n    for epoch in range(EPOCHS):\n        model.train()\n        batch_losses = []\n        \n        for x_b, y_b in train_loader:\n            optimizer.zero_grad()\n            pred = model(x_b)\n            loss = criterion(pred, y_b)\n            loss.backward()\n            optimizer.step()\n            batch_losses.append(loss.item())\n            \n        # Validation\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for x_b, y_b in val_loader:\n                pred = model(x_b)\n                loss = criterion(pred, y_b)\n                val_losses.append(loss.item())\n        \n        avg_val_loss = np.mean(val_losses)\n        scheduler.step(avg_val_loss)\n        \n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            best_state = model.state_dict()\n            \n        if (epoch+1) % 5 == 0:\n            print(f\"Ep {epoch+1}: Train MSE {np.mean(batch_losses):.4f} | Val MSE {avg_val_loss:.4f}\")\n\n    print(f\"Fold {fold+1} Best RMSE: {np.sqrt(best_loss):.4f}\")\n    fold_rmses.append(np.sqrt(best_loss))\n    fold_models.append(best_state)\n\nprint(f\"\\nAverage CV RMSE: {np.mean(fold_rmses):.4f}\")\n\n# 4. Inference (Ensemble)\nprint(\"Generating Ensemble Predictions...\")\nfinal_preds = np.zeros(len(X_test_tensor))\n\nfor state in fold_models:\n    model = RobustRegressor(input_dim=X_train_tensor.shape[1]).to(DEVICE)\n    model.load_state_dict(state)\n    model.eval()\n    with torch.no_grad():\n        preds = model(X_test_tensor).cpu().numpy().flatten()\n        final_preds += preds\n\nfinal_preds /= K_FOLDS\nfinal_preds = np.clip(np.round(final_preds), 0, 10)\n\n# Save (Using 'testing_data' and 'OUTPUT_DIR')\nsubmission = pd.DataFrame({'ID': testing_data['submission_id'], 'score': final_preds})\nsubmission.to_csv(OUTPUT_DIR / 'submission.csv', index=False)\nprint(f\"Submission Saved to {OUTPUT_DIR / 'submission.csv'}!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:23:02.888930Z","iopub.execute_input":"2025-11-21T11:23:02.889203Z","iopub.status.idle":"2025-11-21T11:24:03.774449Z","shell.execute_reply.started":"2025-11-21T11:23:02.889178Z","shell.execute_reply":"2025-11-21T11:24:03.773622Z"}},"outputs":[{"name":"stdout","text":"Starting 5-Fold Cross-Validation...\n\n--- Fold 1 ---\nEp 5: Train MSE 6.8598 | Val MSE 14.2414\nEp 10: Train MSE 2.2823 | Val MSE 14.1534\nEp 15: Train MSE 1.1943 | Val MSE 14.0420\nEp 20: Train MSE 0.9097 | Val MSE 14.2687\nEp 25: Train MSE 0.7817 | Val MSE 14.1586\nFold 1 Best RMSE: 3.7139\n\n--- Fold 2 ---\nEp 5: Train MSE 7.2917 | Val MSE 12.7789\nEp 10: Train MSE 3.4135 | Val MSE 13.1882\nEp 15: Train MSE 1.2233 | Val MSE 12.8477\nEp 20: Train MSE 0.8080 | Val MSE 13.1422\nEp 25: Train MSE 0.6864 | Val MSE 13.0038\nFold 2 Best RMSE: 3.5338\n\n--- Fold 3 ---\nEp 5: Train MSE 6.7987 | Val MSE 13.5544\nEp 10: Train MSE 2.5200 | Val MSE 13.9737\nEp 15: Train MSE 1.2145 | Val MSE 13.9787\nEp 20: Train MSE 0.8485 | Val MSE 13.8351\nEp 25: Train MSE 0.7197 | Val MSE 14.0215\nFold 3 Best RMSE: 3.6816\n\n--- Fold 4 ---\nEp 5: Train MSE 7.1056 | Val MSE 12.4939\nEp 10: Train MSE 2.7367 | Val MSE 12.9170\nEp 15: Train MSE 1.1703 | Val MSE 12.4318\nEp 20: Train MSE 0.8801 | Val MSE 12.6603\nEp 25: Train MSE 0.6896 | Val MSE 12.8470\nFold 4 Best RMSE: 3.5259\n\n--- Fold 5 ---\nEp 5: Train MSE 6.9448 | Val MSE 14.1375\nEp 10: Train MSE 2.2901 | Val MSE 13.6544\nEp 15: Train MSE 1.1878 | Val MSE 13.6675\nEp 20: Train MSE 0.8872 | Val MSE 13.8041\nEp 25: Train MSE 0.6657 | Val MSE 13.6449\nFold 5 Best RMSE: 3.6363\n\nAverage CV RMSE: 3.6183\nGenerating Ensemble Predictions...\nSubmission Saved to /kaggle/working/submission.csv!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from IPython.display import FileLink\nimport os\n\n# Ensure we are in the correct directory\nos.chdir('/kaggle/working')\n\n# Create a clickable download link\nprint(\"Click the link below to download your file:\")\nFileLink(r'submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T11:24:03.775345Z","iopub.execute_input":"2025-11-21T11:24:03.775699Z","iopub.status.idle":"2025-11-21T11:24:03.781385Z","shell.execute_reply.started":"2025-11-21T11:24:03.775672Z","shell.execute_reply":"2025-11-21T11:24:03.780620Z"}},"outputs":[{"name":"stdout","text":"Click the link below to download your file:\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}